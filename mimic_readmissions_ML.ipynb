{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Read-in-preprocessed-patient-data\" data-toc-modified-id=\"Read-in-preprocessed-patient-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Read in preprocessed patient data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Encode-categorical-features-and-scale-numerical-values\" data-toc-modified-id=\"Encode-categorical-features-and-scale-numerical-values-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Encode categorical features and scale numerical values</a></span></li></ul></li><li><span><a href=\"#Read-in-NOTEEVENTS-table\" data-toc-modified-id=\"Read-in-NOTEEVENTS-table-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Read in NOTEEVENTS table</a></span></li><li><span><a href=\"#Merge-notes-table-with-adm_processed-table\" data-toc-modified-id=\"Merge-notes-table-with-adm_processed-table-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Merge notes table with adm_processed table</a></span></li><li><span><a href=\"#Create-training-and-test-dataframes\" data-toc-modified-id=\"Create-training-and-test-dataframes-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Create training and test dataframes</a></span></li><li><span><a href=\"#Preprocess-text-data\" data-toc-modified-id=\"Preprocess-text-data-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Preprocess text data</a></span></li><li><span><a href=\"#Word2Vec-processing\" data-toc-modified-id=\"Word2Vec-processing-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Word2Vec processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Prepare-text-data-for-W2V-modeling\" data-toc-modified-id=\"Prepare-text-data-for-W2V-modeling-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Prepare text data for W2V modeling</a></span></li><li><span><a href=\"#Train-Word2Vec-model\" data-toc-modified-id=\"Train-Word2Vec-model-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Train Word2Vec model</a></span></li></ul></li><li><span><a href=\"#Vectorize-clinic-notes\" data-toc-modified-id=\"Vectorize-clinic-notes-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Vectorize clinic notes</a></span><ul class=\"toc-item\"><li><span><a href=\"#Vectorize-notes-and-store-as-text-data-frame\" data-toc-modified-id=\"Vectorize-notes-and-store-as-text-data-frame-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Vectorize notes and store as text data frame</a></span></li><li><span><a href=\"#Append-vectorized-notes-to-train-and-test-X-dataframes\" data-toc-modified-id=\"Append-vectorized-notes-to-train-and-test-X-dataframes-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Append vectorized notes to train and test X dataframes</a></span></li></ul></li><li><span><a href=\"#SMOTE-Balancing\" data-toc-modified-id=\"SMOTE-Balancing-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>SMOTE Balancing</a></span></li><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Modeling</a></span></li><li><span><a href=\"#Oversample-the-minority-class\" data-toc-modified-id=\"Oversample-the-minority-class-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Oversample the minority class</a></span><ul class=\"toc-item\"><li><span><a href=\"#Train-2-models\" data-toc-modified-id=\"Train-2-models-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Train 2 models</a></span></li><li><span><a href=\"#Pickle-all-of-the-models-we-need-for-the-dashboard\" data-toc-modified-id=\"Pickle-all-of-the-models-we-need-for-the-dashboard-10.2\"><span class=\"toc-item-num\">10.2&nbsp;&nbsp;</span>Pickle all of the models we need for the dashboard</a></span></li></ul></li><li><span><a href=\"#Try-random-forest-on-non-normalized-values\" data-toc-modified-id=\"Try-random-forest-on-non-normalized-values-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Try random forest on non-normalized values</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# For interacting with PostgreSQL database for mimic queries\n",
    "import psycopg2\n",
    "\n",
    "# Ploting functions\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "plotly.tools.set_credentials_file(username='mlpaff', api_key='lYV8hhGxZlP988tplymj')\n",
    "plotly.tools.set_config_file(world_readable=True,\n",
    "                             sharing='public')\n",
    "\n",
    "from IPython.core.pylabtools import figsize\n",
    "import matplotlib.pyplot as plt\n",
    "figsize(20, 10)\n",
    "plt.style.use(['dark_background'])\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "color_set = ['#A9CA59', '#6582C4', '#62C9BC', '#F58D50', '#2AD7F4',\n",
    "             '#AB3EED', '#FF6CB2', '#FFA466', '#FFE256', '#47EAAC', '#2AD7F4', '#3C8CF9']\n",
    "\n",
    "# specify user and database for SQL queries\n",
    "sqluser = 'mattmimic'\n",
    "dbname = 'mimic'\n",
    "set_schema = '--search_path=mimiciii'\n",
    "\n",
    "# Connect to the database\n",
    "# con = psycopg2.connect(dbname = dbname, user = sqluser, options = set_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in preprocessed patient data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm_processed = pd.read_csv('./data/admission_processed.csv', parse_dates=['admittime', 'dischtime', 'deathtime', 'edregtime', 'edouttime', 'next_admittime', 'dob'], date_parser=pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the structured features that will be included in the model\n",
    "feature_set_1 = ['admission_type', 'total_prior_admits','gender', 'age', 'length_of_stay', 'num_medications', 'num_lab_tests', 'perc_tests_abnormal', 'num_diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only those features that we want and add in hadm_id (to merge notes on)\n",
    "adm_processed = adm_processed[['hadm_id', 'subject_id', 'days_next_admit'] + feature_set_1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode categorical features and scale numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning:\n",
      "\n",
      "Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Defining dictionaries for encoding\n",
    "admin_type_dict = {'EMERGENCY': 0, 'URGENT': 0, 'ELECTIVE': 1}\n",
    "gender_dict = {'M': 0, 'F': 1}\n",
    "\n",
    "# Mapping dictionaries to binary features\n",
    "adm_processed['admission_type'] = adm_processed['admission_type'].map(admin_type_dict).astype(int)\n",
    "adm_processed['gender'] = adm_processed['gender'].map(gender_dict).astype(int)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = MinMaxScaler()\n",
    "standard_cols = ['total_prior_admits', 'age', 'length_of_stay', 'num_medications', 'num_lab_tests', 'num_diagnosis']\n",
    "\n",
    "# Normalizing the numeric columns\n",
    "adm_processed[standard_cols] = scaler.fit_transform(adm_processed[standard_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in NOTEEVENTS table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>chartdate</th>\n",
       "      <th>charttime</th>\n",
       "      <th>storetime</th>\n",
       "      <th>category</th>\n",
       "      <th>description</th>\n",
       "      <th>cgid</th>\n",
       "      <th>iserror</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>804333</td>\n",
       "      <td>5289</td>\n",
       "      <td>194762.0</td>\n",
       "      <td>2110-11-05</td>\n",
       "      <td>2110-11-05 06:52:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[**2110-11-5**] 6:52 AM\\n CHEST (PORTABLE AP) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804334</td>\n",
       "      <td>13993</td>\n",
       "      <td>180704.0</td>\n",
       "      <td>2103-11-07</td>\n",
       "      <td>2103-11-07 06:53:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[**2103-11-7**] 6:53 AM\\n CHEST (PORTABLE AP) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>804467</td>\n",
       "      <td>4599</td>\n",
       "      <td>109574.0</td>\n",
       "      <td>2120-10-31</td>\n",
       "      <td>2120-10-31 12:37:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>CT PERITONEAL DRAINAGE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[**2120-10-31**] 12:37 PM\\n CT PERITONEAL DRAI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>804108</td>\n",
       "      <td>9090</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2180-09-25</td>\n",
       "      <td>2180-09-25 08:20:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>UGI SGL CONTRAST W/ KUB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[**2180-9-25**] 8:20 AM\\n UGI SGL CONTRAST W/ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>804109</td>\n",
       "      <td>19621</td>\n",
       "      <td>102739.0</td>\n",
       "      <td>2193-09-23</td>\n",
       "      <td>2193-09-23 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>PERSANTINE MIBI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>PERSANTINE MIBI                               ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  subject_id   hadm_id  chartdate           charttime storetime  \\\n",
       "0  804333        5289  194762.0 2110-11-05 2110-11-05 06:52:00       NaT   \n",
       "1  804334       13993  180704.0 2103-11-07 2103-11-07 06:53:00       NaT   \n",
       "2  804467        4599  109574.0 2120-10-31 2120-10-31 12:37:00       NaT   \n",
       "3  804108        9090       NaN 2180-09-25 2180-09-25 08:20:00       NaT   \n",
       "4  804109       19621  102739.0 2193-09-23 2193-09-23 00:00:00       NaT   \n",
       "\n",
       "    category              description  cgid iserror  \\\n",
       "0  Radiology      CHEST (PORTABLE AP)   NaN    None   \n",
       "1  Radiology      CHEST (PORTABLE AP)   NaN    None   \n",
       "2  Radiology   CT PERITONEAL DRAINAGE   NaN    None   \n",
       "3  Radiology  UGI SGL CONTRAST W/ KUB   NaN    None   \n",
       "4  Radiology          PERSANTINE MIBI   NaN    None   \n",
       "\n",
       "                                                text  \n",
       "0  [**2110-11-5**] 6:52 AM\\n CHEST (PORTABLE AP) ...  \n",
       "1  [**2103-11-7**] 6:53 AM\\n CHEST (PORTABLE AP) ...  \n",
       "2  [**2120-10-31**] 12:37 PM\\n CT PERITONEAL DRAI...  \n",
       "3  [**2180-9-25**] 8:20 AM\\n UGI SGL CONTRAST W/ ...  \n",
       "4  PERSANTINE MIBI                               ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "con = psycopg2.connect(dbname = dbname, user = sqluser, options = set_schema)\n",
    "query = 'SELECT * FROM noteevents;'\n",
    "notes = pd.read_sql_query(query, con)\n",
    "con.close()\n",
    "notes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Multiple discharge summaries per admission",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9ddbb2abef34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdis_notes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnotes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnotes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Discharge summary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mdis_notes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hadm_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Multiple discharge summaries per admission'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Multiple discharge summaries per admission"
     ]
    }
   ],
   "source": [
    "# Filter to discharge summary notes only\n",
    "dis_notes = notes[notes['category'] == 'Discharge summary'].copy()\n",
    "\n",
    "assert dis_notes.duplicated(['hadm_id']).sum() == 0, 'Multiple discharge summaries per admission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab just the last discharge summaries by hadm_id\n",
    "last_dis_notes = dis_notes.groupby(['subject_id', 'hadm_id']).nth(-1).reset_index()\n",
    "assert last_dis_notes.duplicated(['hadm_id']).sum() == 0, 'Multiple discharge summaries per admission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_dis_notes.head()\n",
    "note_features = ['subject_id', 'hadm_id', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>days_next_admit</th>\n",
       "      <th>admission_type</th>\n",
       "      <th>total_prior_admits</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>length_of_stay</th>\n",
       "      <th>num_medications</th>\n",
       "      <th>num_lab_tests</th>\n",
       "      <th>perc_tests_abnormal</th>\n",
       "      <th>num_diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>185777</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.523442</td>\n",
       "      <td>0.026332</td>\n",
       "      <td>0.043219</td>\n",
       "      <td>0.017723</td>\n",
       "      <td>0.240816</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107064</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.721418</td>\n",
       "      <td>0.055537</td>\n",
       "      <td>0.109538</td>\n",
       "      <td>0.041645</td>\n",
       "      <td>0.448517</td>\n",
       "      <td>0.205128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>194540</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.548635</td>\n",
       "      <td>0.086639</td>\n",
       "      <td>0.067064</td>\n",
       "      <td>0.032091</td>\n",
       "      <td>0.104072</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>143045</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.436122</td>\n",
       "      <td>0.023266</td>\n",
       "      <td>0.061848</td>\n",
       "      <td>0.026037</td>\n",
       "      <td>0.298050</td>\n",
       "      <td>0.128205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>194023</td>\n",
       "      <td>17</td>\n",
       "      <td>128.920833</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.519159</td>\n",
       "      <td>0.014824</td>\n",
       "      <td>0.040238</td>\n",
       "      <td>0.013857</td>\n",
       "      <td>0.296875</td>\n",
       "      <td>0.102564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hadm_id  subject_id  days_next_admit  admission_type  total_prior_admits  \\\n",
       "0   185777           4              NaN               0                 0.0   \n",
       "1   107064           6              NaN               1                 0.0   \n",
       "2   194540          11              NaN               0                 0.0   \n",
       "3   143045          13              NaN               0                 0.0   \n",
       "4   194023          17       128.920833               1                 0.0   \n",
       "\n",
       "   gender       age  length_of_stay  num_medications  num_lab_tests  \\\n",
       "0       1  0.523442        0.026332         0.043219       0.017723   \n",
       "1       1  0.721418        0.055537         0.109538       0.041645   \n",
       "2       1  0.548635        0.086639         0.067064       0.032091   \n",
       "3       1  0.436122        0.023266         0.061848       0.026037   \n",
       "4       1  0.519159        0.014824         0.040238       0.013857   \n",
       "\n",
       "   perc_tests_abnormal  num_diagnosis  \n",
       "0             0.240816       0.230769  \n",
       "1             0.448517       0.205128  \n",
       "2             0.104072       0.025641  \n",
       "3             0.298050       0.128205  \n",
       "4             0.296875       0.102564  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adm_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Admission Date:  [**2178-4-16**]              Discharge Date:   [**2178-5-11**]\\n\\nDate of Birth:  [**2128-2-22**]             Sex:   F\\n\\nService: NEUROSURGERY\\n\\nAllergies:\\nPenicillins\\n\\nAttending:[**First Name3 (LF) 1854**]\\nChief Complaint:\\nCC:[**CC Contact Info 71794**]\\n\\nMajor Surgical or Invasive Procedure:\\nSTEREOTACTIC BRAIN BIOPSY, Neuronavigation guided tumor\\nresection.\\n\\n\\nHistory of Present Illness:\\nHPI: 50 year old female presents after having fallen in the\\nbathtub 4 days ago and hitting the back of her head. Since then\\nshe has had a \"massive headache\" which did not resolve with\\nTylenol. She states that she has a high threshold for pain and\\ndid not realize how bad it was during the day while at work but\\nthen when she got home at night she noticed it. The patient\\nnoticed \"silvery spects\" in her vision and she had trouble with\\nsome simple tasks like finding the tags on the back of her\\nclothing in the morning. She reported that she had to check\\nseveral times to make sure she did not put her clothes on\\nbackwards. She has had some dizziness, but no nausea or\\nvomiting.\\nHer speech has not been affected.\\n\\n\\nPast Medical History:\\nNewly diagnosed GBM as above\\notherwise, none\\n\\n\\nFamily History:\\n\\nPhysical Exam:\\nON ADMISSION:\\n\\nPHYSICAL EXAM:\\nT:98.4  BP:105/55    HR:95    RR:15     O2Sats: 98% RA\\nGen: WD/WN, comfortable, NAD.\\nHEENT: Pupils: PERRL       EOMs-intact\\nNeck: Supple.\\nLungs: CTA bilaterally.\\nCardiac: RRR. S1/S2.\\nAbd: Soft, NT, BS+\\nExtrem: Warm and well-perfused.\\nNeuro:\\nMental status: Awake and alert, cooperative with exam, normal\\naffect.\\nOrientation: Oriented to person, place, and date.\\nLanguage: Speech fluent with good comprehension and repetition.\\nNaming intact. No dysarthria or paraphasic errors.\\n\\nCranial Nerves:\\nI: Not tested\\nII: Pupils equally round and reactive to light, 3 to 2 mm\\nbilaterally. Visual fields are full to confrontation.\\nIII, IV, VI: Extraocular movements intact bilaterally without\\nnystagmus.\\nV, VII: Facial strength and sensation intact and symmetric.\\nVIII: Hearing intact to voice.\\nIX, X: Palatal elevation symmetrical.\\n[**Doctor First Name 81**]: Sternocleidomastoid and trapezius normal bilaterally.\\nXII: Tongue midline without fasciculations.\\n\\nMotor: Normal bulk and tone bilaterally. No abnormal movements,\\ntremors. Strength full power [**5-9**] throughout. No pronator drift.\\n\\nWhen asked to rotate fists around each other, her right fist\\norbits the left, which may show slight LUE weakness.\\n\\nSensation: Intact to light touch bilaterally.\\n\\nReflexes: intact\\n\\nToes downgoing bilaterally\\n\\nCoordination: normal on finger-nose-finger\\n\\n\\nPertinent Results:\\nCT HEAD W/O CONTRAST  [**2178-4-21**] 12:28 PM\\n\\nCT HEAD W/O CONTRAST\\n\\nReason: please evaluate for any new changes\\n\\n[**Hospital 93**] MEDICAL CONDITION:\\n50 year old woman with brain mass. Now has headache.\\nREASON FOR THIS EXAMINATION:\\nplease evaluate for any new changes\\nCONTRAINDICATIONS for IV CONTRAST: None.\\n\\nNON-CONTRAST HEAD CT SCAN\\n\\nHISTORY: Brain mass. Now has headache. Evaluate for any changes.\\n\\nTECHNIQUE: Non-contrast head CT scan.\\n\\nCOMPARISON STUDY: [**2178-4-17**] CT scan of the head, reported\\nby Drs. [**Last Name (STitle) 21881**] and [**Name5 (PTitle) **] as revealing \"unchanged mass\\neffect and edema around large right parietal mass, without\\nevidence of new intracranial hemorrhage following biopsy.\"\\n\\nFINDINGS: Since the prior study, there is now mild linear\\nhyperdensity within the basal cisterns of this could be\\nhemorrhage, occasionally the tributaries of the circle of [**Location (un) 431**]\\ncan be somewhat denser appearing, in the setting of increased\\nintracranial pressure, which would mimic the presence of\\nsubarachnoid blood.\\n\\nNo other new intracranial or extracranial abnormalities are\\ndiscerned.\\n\\nCONCLUSION: Possible small amount of subarachnoid blood or\\nrelatively stagnant vascular flow, in the face of increased\\nintracranial pressure, as noted above. We have telephoned (Dr.\\n[**Last Name (STitle) **] with this report immediately after the conclusion of the\\nstudy.\\n\\nADDENDUM: There is a tiny residual gas collection superior to\\nthe biopsy site, definitely decreased in extent compared to the\\nprior study of [**4-17**].\\n\\nDR. [**First Name4 (NamePattern1) **] [**Last Name (NamePattern1) 9987**]\\nCardiology Report ECG Study Date of [**2178-4-16**]  3:21:48 PM\\n\\n\\nNormal sinus rhythm. Within normal limits. No previous tracing\\navailable for\\ncomparison.\\n\\n\\nRead by: [**Last Name (LF) **],[**First Name8 (NamePattern2) 2206**] [**Doctor Last Name **]\\n\\n  Intervals Axes\\nRate PR QRS QT/QTc P QRS T\\n90 142 82 [**Telephone/Fax (2) 71795**] 45 48\\n\\n\\n([**-7/2007**])\\n\\nMR HEAD W & W/O CONTRAST  [**2178-4-16**] 5:01 AM\\n\\nMR HEAD W & W/O CONTRAST\\n\\nReason: evaluate extent of ring enhancing brain mass w/ edema\\nseen o\\nContrast: MAGNEVIST\\n\\n[**Hospital 93**] MEDICAL CONDITION:\\n50 year old woman with brain mass\\nREASON FOR THIS EXAMINATION:\\nevaluate extent of ring enhancing brain mass w/ edema seen on CT\\n\\nEMERGENCY MRI SCAN OF THE BRAIN.\\n\\nHISTORY: Ring-enhancing brain mass with edema seen on CT scan.\\n\\nTECHNIQUE: Multiplanar T1 and T2-weighted brain images with\\ngadolinium enhancement.\\n\\nCOMPARISON STUDIES: None available at this time.\\n\\nWET READ REPORT: Dr. [**Last Name (STitle) 71796**] interpreted this study as\\nrevealing \"bilateral parenchymal masses, suggesting metastatic\\ndisease. The largest mass in the right parietal lobe has a\\ncomplex appearance, at least 5 cm in greatest dimension with at\\nleast 8 cm greatest dimension of surrounding edema and 7 mm\\nleftward midline shift.\" Dr. [**Last Name (STitle) 71796**] is a member of the\\n\"Nighthawk\" Radiology group.\\n\\nFINDINGS: The study indeed reveals a large right parietal,\\nirregularly thick ring enhancing mass, possibly with some tiny\\n\"daughter\" cystic components extending towards the cortical\\nsurface. As mentioned in the wet [**Location (un) 1131**], there is a substantial\\narea of surrounding edema, with effacement of the right atrium\\nand approximately 7 mm leftward subfalcine herniation. A smaller\\narea of edema is seen within the white matter of the left\\noccipital lobe, but I cannot delineate any specific area of\\nenhancement associated with it. A third, very well circumscribed\\n18 x 26 mm area of elevated T2 signal is seen contiguous to the\\nleft temporal lobe, and it is difficult to determine whether the\\nlesion is intra- or extra-axial in locale. Again, there is no\\nassociated enhancement and no abnormal susceptibility is noted,\\neither. On the coronal post-contrast images, there is a\\nquestionable area of enhancement, approximately 3 mm,\\nimmediately subjacent to the left temporal lesion- I am not\\ncertain that the two findings are necessarily related. The\\nprincipal vascular flow patterns are identified. There is no\\novert extracranial abnormality noted.\\n\\nCONCLUSION: Large right parietal lobe mass, which may represent\\neither a primary or metastatic brain neoplasm. The additional\\nabnormalities within the left temporal lobe and left occipital\\nlobe, while they may represent extremely unusual manifestations\\nof metastatic disease, which would then render the right\\nparietal lobe lesion more likely metastatic, could have\\nalternative diagnoses, including hemorrhage or a calcified left\\ntemporal meningioma as an explanation for the left temporal\\nlesion, and either an inflammatory or ischemic process to\\naccount for the left occipital lesion. I discussed this case\\nthis morning with Dr. [**Last Name (STitle) **], it was decided that a non-contrast\\nhead CT scan would be a helpful followup diagnostic procedure to\\nfurther characterize the left cerebral hemispheric lesions prior\\nto brain biopsy of the right parietal lesion.\\n\\nDR. [**First Name4 (NamePattern1) **] [**Last Name (NamePattern1) 9987**]\\nApproved: [**Doctor First Name **] [**2178-4-16**] 4:07 PM\\n\\n CHEST (PORTABLE AP)  [**2178-4-16**] 3:50 PM\\n\\nCHEST (PORTABLE AP)\\n\\nReason: please evaluate preop\\n\\n[**Hospital 93**] MEDICAL CONDITION:\\n50 year old woman with bilateral masses\\nREASON FOR THIS EXAMINATION:\\nplease evaluate preop\\nINDICATION: Bilateral brain masses for pre-op evaluation.\\n\\nPORTABLE CHEST: There are no priors for comparison. Heart size\\nis normal. Mediastinal and hilar contours are normal. Pulmonary\\nvasculature is normal. Lungs are clear, and there are no\\neffusions or pneumothorax.\\n\\nIMPRESSION:\\n1. No acute cardiopulmonary disease.\\n2. No evidence of a primary pulmonary neoplasm, but PA/Lateral\\nCXR or CT would be more sensitive than a portable study and may\\nbe helpful for more complete assessment, if not already recently\\nobtained.\\n\\nThe study and the report were reviewed by the staff radiologist.\\nDR. [**First Name8 (NamePattern2) **] [**Last Name (NamePattern1) 3904**]\\nDR. [**First Name11 (Name Pattern1) **] [**Initial (NamePattern1) **] [**Last Name (NamePattern4) 5785**]\\nApproved: FRI [**2178-4-17**] 9:02 AM\\n\\n\\n****\\n\\n\\n\\n*******\\n\\nBrief Hospital Course:\\nA/P: 50 year-old woman without significant PMH who presented\\n[**2178-4-16**] from OSH with new dx of brain mass after fall, with\\nbiopsy that showed GBM, for which she had craniectomy/debulking\\n[**2178-4-23**] and was started on XRT without event.\\n.\\n1) Glioblastoma Multiforme: She was transferred to [**Hospital1 18**] after\\nfall and was noted to have a brain mass. She had a biopsy which\\nshowed glioblastoma, WHO grade IV. She was noted on [**4-21**] to have\\nheadache, with 4/18 she had mental status changes and imaging\\nc/w herniation. She was taken emergently to the OR [**4-23**] for\\ncraniotomy and subtotal tumor debulking. She was then\\ntransferred to Omed for XRT and chemotherapy. She was maintained\\non keppra and decadron for seizure/cerebral edema prevention.\\nShe was maintained on seizure precautions with frequent neuro\\nchecks. She used a helmet with ambulation given s/p craniectomy.\\nShe was started on XRT with temador. She remained stable without\\nsigns of elevated intracranial pressure during this so was\\nthought stable to go home and continue XRT as an outpatient. She\\nwas seen by PT and though stable to go home. She was continued\\non pantoprazole and sliding scale insulin while on\\ndexamethasone.\\n.\\n\\n\\nMedications on Admission:\\nnone\\n\\nDischarge Medications:\\n1. Diabetic.com Starter Kit     Kit Sig: One (1) kit\\nMiscellaneous once a day.\\nDisp:*1 kit* Refills:*0*\\n2. Insulin Lispro (Human) 100 unit/mL Solution Sig: 0-15 units\\nSubcutaneous ASDIR (AS DIRECTED): per sliding scale, check blood\\nglucose 4 times daily.\\nDisp:*QS units* Refills:*2*\\n3. Docusate Sodium 100 mg Capsule Sig: One (1) Capsule PO BID (2\\ntimes a day).\\nDisp:*60 Capsule(s)* Refills:*2*\\n4. Senna 8.6 mg Tablet Sig: One (1) Tablet PO BID (2 times a\\nday).\\nDisp:*60 Tablet(s)* Refills:*2*\\n5. Dexamethasone 4 mg Tablet Sig: Two (2) Tablet PO Q8H (every 8\\nhours).\\nDisp:*90 Tablet(s)* Refills:*2*\\n6. Pantoprazole 40 mg Tablet, Delayed Release (E.C.) Sig: One\\n(1) Tablet, Delayed Release (E.C.) PO Q24H (every 24 hours).\\nDisp:*30 Tablet, Delayed Release (E.C.)(s)* Refills:*2*\\n7. Levetiracetam 500 mg Tablet Sig: Three (3) Tablet PO BID (2\\ntimes a day).\\nDisp:*180 Tablet(s)* Refills:*2*\\n8. Oxycodone 5 mg Tablet Sig: One (1) Tablet PO Q4-6H (every 4\\nto 6 hours) as needed for pain.\\nDisp:*20 Tablet(s)* Refills:*0*\\n9. Trazodone 50 mg Tablet Sig: 0.5 Tablet PO HS (at bedtime) as\\nneeded for insomnia.\\nDisp:*15 Tablet(s)* Refills:*2*\\n10. Ondansetron 4 mg Tablet, Rapid Dissolve Sig: Two (2) Tablet,\\nRapid Dissolve PO DAILY (Daily): 2 pills 30 minutes prior to\\nTemodar (chemotherapy) or [**1-6**] pills as needed every 8 hours for\\nnausea.\\nDisp:*90 Tablet, Rapid Dissolve(s)* Refills:*2*\\n11. Omega-3 Fatty Acids 550 mg Capsule Sig: One (1) Capsule PO\\nBID (2 times a day).\\n12. Melatonin  Oral\\n\\n\\nDischarge Disposition:\\nHome With Service\\n\\nFacility:\\n[**Hospital **] Hospice and VNA\\n\\nDischarge Diagnosis:\\nGLIOBLASTOMA\\n\\n\\nDischarge Condition:\\nSTABLE\\n\\n\\nDischarge Instructions:\\nPlease take all medications as prescribed. Please keep all\\nfollow-up appointments. You will be contact[**Name (NI) **] by Dr. [**First Name8 (NamePattern2) **] [**Last Name (NamePattern1) 724**]\\nregarding your Temodar prescription.\\n.\\nPlease call your primary care physician or Dr. [**First Name (STitle) **] [**Name (STitle) 4253**]\\nif you experience headaches, visual changes, nasea, vomitting,\\nhiccups, change in strength, sensation, or coordination. These\\ncould be signs of elevated intercranial pressure and could\\nrequire urgent treatment.\\n*\\nPlease limit exercise to walking; no lifting, straining,\\nexcessive bending.\\nPlease continue to use your helmet with ambulation.\\nYou may wash your hair only after sutures and/or staples have\\nbeen removed. You may shower before this time with assistance\\nand use of a shower cap.\\nIncrease your intake of fluids and fiber as pain medicine\\n(narcotics) can cause constipation.\\nUnless directed by your doctor, do not take any\\nanti-inflammatory medicines such as Motrin, aspirin, Advil,\\nIbuprofen etc.\\nCALL YOUR SURGEON IMMEDIATELY IF YOU EXPERIENCE ANY OF THE\\nFOLLOWING:\\n\\n??????\\tNew onset of tremors or seizures\\n??????\\tAny confusion or change in mental status\\n??????\\tAny numbness, tingling, weakness in your extremities\\n??????\\tPain or headache that is continually increasing or not\\nrelieved by pain medication\\n??????\\tAny signs of infection at the wound site: redness, swelling,\\ntenderness, drainage\\n??????\\tFever greater than or equal to 101?????? F\\n\\n\\nFollowup Instructions:\\nPlease continue with your daily radiation therapy treatments: to\\ncontinue this as an outpatient you will need to call: ([**Telephone/Fax (1) 54862**] first thing in the morning of [**2178-5-12**]. If you have\\nany difficulty schduling these treatments or questions please\\ncall ([**Telephone/Fax (1) 71797**] and ask for [**First Name4 (NamePattern1) **] [**Last Name (NamePattern1) **].\\n.\\nPlease follow-up with Dr. [**First Name (STitle) **] [**Name (STitle) 4253**] on [**Last Name (LF) 766**], [**2178-5-18**]. You should be contact[**Name (NI) **] with the time of this appointment\\nbut if you do not hear from her office please call [**Telephone/Fax (1) 1844**].\\n\\n.\\nPlease call [**Telephone/Fax (1) 28193**] to schedule follow-up with your primary\\ncare physician, [**Last Name (NamePattern4) **]. [**Last Name (STitle) 28190**] [**Name (STitle) 16528**] in the next 2 weeks.\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_dis_notes[last_dis_notes['hadm_id'] == 194540].text.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge notes table with adm_processed table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge notes table with processed structural data\n",
    "adm_notes = adm_processed.merge(last_dis_notes[note_features], on = ['subject_id', 'hadm_id'], how = 'left')\n",
    "\n",
    "# assert len(admissions) == len(adm_notes), 'Number of rows increased'\n",
    "\n",
    "# Generate output label for readmissions under 30 days\n",
    "adm_notes['output_label'] = (adm_notes['days_next_admit'] < 30).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>days_next_admit</th>\n",
       "      <th>admission_type</th>\n",
       "      <th>total_prior_admits</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>length_of_stay</th>\n",
       "      <th>num_medications</th>\n",
       "      <th>num_lab_tests</th>\n",
       "      <th>perc_tests_abnormal</th>\n",
       "      <th>num_diagnosis</th>\n",
       "      <th>text</th>\n",
       "      <th>output_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>185777</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.523442</td>\n",
       "      <td>0.026332</td>\n",
       "      <td>0.043219</td>\n",
       "      <td>0.017723</td>\n",
       "      <td>0.240816</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>Admission Date:  [**2191-3-16**]     Discharge...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107064</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.721418</td>\n",
       "      <td>0.055537</td>\n",
       "      <td>0.109538</td>\n",
       "      <td>0.041645</td>\n",
       "      <td>0.448517</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>Admission Date: [**2175-5-30**]        Dischar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>194540</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.548635</td>\n",
       "      <td>0.086639</td>\n",
       "      <td>0.067064</td>\n",
       "      <td>0.032091</td>\n",
       "      <td>0.104072</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>Admission Date:  [**2178-4-16**]              ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>143045</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.436122</td>\n",
       "      <td>0.023266</td>\n",
       "      <td>0.061848</td>\n",
       "      <td>0.026037</td>\n",
       "      <td>0.298050</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>Name:  [**Known lastname 9900**], [**Known fir...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>194023</td>\n",
       "      <td>17</td>\n",
       "      <td>128.920833</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.519159</td>\n",
       "      <td>0.014824</td>\n",
       "      <td>0.040238</td>\n",
       "      <td>0.013857</td>\n",
       "      <td>0.296875</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>Admission Date:  [**2134-12-27**]             ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hadm_id  subject_id  days_next_admit  admission_type  total_prior_admits  \\\n",
       "0   185777           4              NaN               0                 0.0   \n",
       "1   107064           6              NaN               1                 0.0   \n",
       "2   194540          11              NaN               0                 0.0   \n",
       "3   143045          13              NaN               0                 0.0   \n",
       "4   194023          17       128.920833               1                 0.0   \n",
       "\n",
       "   gender       age  length_of_stay  num_medications  num_lab_tests  \\\n",
       "0       1  0.523442        0.026332         0.043219       0.017723   \n",
       "1       1  0.721418        0.055537         0.109538       0.041645   \n",
       "2       1  0.548635        0.086639         0.067064       0.032091   \n",
       "3       1  0.436122        0.023266         0.061848       0.026037   \n",
       "4       1  0.519159        0.014824         0.040238       0.013857   \n",
       "\n",
       "   perc_tests_abnormal  num_diagnosis  \\\n",
       "0             0.240816       0.230769   \n",
       "1             0.448517       0.205128   \n",
       "2             0.104072       0.025641   \n",
       "3             0.298050       0.128205   \n",
       "4             0.296875       0.102564   \n",
       "\n",
       "                                                text  output_label  \n",
       "0  Admission Date:  [**2191-3-16**]     Discharge...             0  \n",
       "1  Admission Date: [**2175-5-30**]        Dischar...             0  \n",
       "2  Admission Date:  [**2178-4-16**]              ...             0  \n",
       "3  Name:  [**Known lastname 9900**], [**Known fir...             0  \n",
       "4  Admission Date:  [**2134-12-27**]             ...             0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adm_notes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of admissions without notes: 0.0232\n",
      "number of patients that were re-admitted within 30 days: 2779\n",
      "fraction of patients re-admitted within 30 days: 0.0672441745106105\n"
     ]
    }
   ],
   "source": [
    "print('Fraction of admissions without notes:', round(adm_notes.text.isnull().sum() / len(adm_notes), 4))\n",
    "print('number of patients that were re-admitted within 30 days:', len(adm_notes[adm_notes['output_label'] == 1]))\n",
    "print('fraction of patients re-admitted within 30 days:', len(adm_notes[adm_notes['output_label'] == 1]) / len(adm_notes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training and test dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33061, 10) (8266, 10) (33061, 1) (8266, 1)\n"
     ]
    }
   ],
   "source": [
    "# shuffle the samples\n",
    "adm_notes = adm_notes.sample(n = len(adm_notes), random_state=42)\n",
    "adm_notes.reset_index(drop=True, inplace=True)\n",
    "\n",
    "target = adm_notes[['output_label']]\n",
    "data = adm_notes[feature_set_1 + ['text']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state = 0)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df):\n",
    "    ''' Preprocesses the text by filling not a number and replacing new lines ('\\n') and carriage returns ('\\r')\n",
    "    '''\n",
    "    \n",
    "    df['text'] = df['text'].fillna(' ')\n",
    "    df['text'] = df['text'].str.replace('\\n', ' ')\n",
    "    df['text'] = df['text'].str.replace('\\r', ' ')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_train = preprocess_text(X_test), preprocess_text(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import word2vec\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare text data for W2V modeling\n",
    "- Here we want to convert everything to lowercase and convert to list of sentences while droping stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = ['the','and','to','of','was','with','a','on','in','for','name',\n",
    "                 'is','patient','s','he','at','as','or','one','she','his','her','am',\n",
    "                 'were','you','pt','pm','by','be','had','your','this','date',\n",
    "                'from','there','an','that','p','are','have','has','h','but','o',\n",
    "                'namepattern','which','every','also', 'b', 'i', 'd', 'admission', 'q', 't']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2vTokenizer(sentence):\n",
    "    ''' Tokenize the text by replacing punctuations and numbers with spaces and lowercase all words\n",
    "    '''\n",
    "    punc_list = string.punctuation + '0123456789'\n",
    "    t = str.maketrans(dict.fromkeys(punc_list, ' '))\n",
    "    text = str(sentence).lower().translate(t)\n",
    "    tokens = [x for x in nltk.word_tokenize(text.strip()) if x not in my_stop_words]\n",
    "#     tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def notes_to_sentences(notes, tokenizer, remove_stopwords=False):\n",
    "    ''' Split text data into tokenized list of sentences\n",
    "    '''\n",
    "    try:\n",
    "        # use NLTK tokenizer to split the text into sentences\n",
    "        raw_sentences = tokenizer.tokenize(notes)\n",
    "        \n",
    "        # Loop over each sentence\n",
    "        sentences = []\n",
    "        for sent in raw_sentences:\n",
    "            # if sentence is empty, skip it\n",
    "            if len(sent) > 0:\n",
    "                tokens = [x for x in w2vTokenizer(sent.strip()) if x not in my_stop_words]\n",
    "                if len(tokens) > 0:\n",
    "                    sentences.append(tokens)\n",
    "        # Return the list of sentences\n",
    "        return sentences\n",
    "    except:\n",
    "        print('nope')\n",
    "        \n",
    "def prepareW2Vtext(notes_list):\n",
    "    ''' From the text corpus (list of tokenized sentences generated from all text data), Tokenize the data\n",
    "    '''\n",
    "    \n",
    "    sentences = []\n",
    "    for note in notes_list:\n",
    "        note = str(note)\n",
    "        if len(note) > 0:\n",
    "            sentences += notes_to_sentences(note, tokenizer)\n",
    "    return(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_list = list(X_train['text'])\n",
    "processed_text = prepareW2Vtext(notes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3354537\n"
     ]
    }
   ],
   "source": [
    "print(len(processed_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 400      # Word vector dimentionality\n",
    "min_word_count = 50     # min word count\n",
    "num_workers = 4         # number of threads to run in parallel\n",
    "context = 4             # Context window size\n",
    "downsampling = 1e-3     # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vModel = Word2Vec(processed_text, workers=num_workers, \\\n",
    "                          size=num_features, min_count=min_word_count, \\\n",
    "                          window=context, sample=downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v = dict(zip(w2vModel.wv.index2word, w2vModel.wv.vectors))\n",
    "\n",
    "# w2vModel.wv.save_word2vec_format('./models/mimic_w2v_model.bin', binary=True)\n",
    "\n",
    "# Load model\n",
    "# model = Word2Vec.load('mimic_w2v_model.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize clinic notes\n",
    "- Using the Word2Vec model trained on the clinic notes corpus, vectorize each patients discharge summary notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_clinic_notes(note):\n",
    "    ''' Tokenize the patient text by replacing punctuations and numbers with spaces and lowercase all words\n",
    "    '''\n",
    "    punc_list = string.punctuation + '0123456789'\n",
    "    t = str.maketrans(dict.fromkeys(punc_list, ' '))\n",
    "    text = str(note).lower().translate(t)\n",
    "#     tokens = (x for x in word_tokenize(text.strip()) if x not in my_stop_words)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        transformed_X = []\n",
    "        for document in X:\n",
    "            doc = [word for word in document if word in self.vocab]\n",
    "            transformed_X.append(doc)\n",
    "        return transformed_X\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "            \n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    ''' Convert notes to vector\n",
    "    '''\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.wv.vectors[0])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "#         doc = [word for word in X if word in self.word2vec.wv.vocab]\n",
    "        X = MyTokenizer(self.word2vec.wv.vocab).fit_transform(X)\n",
    "    \n",
    "        return np.array([\n",
    "                    np.mean([self.word2vec.wv[w] for w in document] or \n",
    "                            [np.zeros(self.dim)], axis = 0) for document in X\n",
    "        ])\n",
    "#         return np.mean(self.word2vec[doc], axis = 0)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the clinic notes so that they can be vectorized: Do this for X_train and X_test\n",
    "X_train['tokens'], X_test['tokens'] = X_train['text'].apply(lambda x: tokenize_clinic_notes(x)), X_test['text'].apply(lambda x: tokenize_clinic_notes(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize notes and store as text data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vVectorizer = MeanEmbeddingVectorizer(w2vModel)\n",
    "\n",
    "# Get the transformed, vectorized text data\n",
    "X_train_vectors, X_test_vectors = pd.DataFrame(w2vVectorizer.fit_transform(X_train['tokens'])), pd.DataFrame(w2vVectorizer.fit_transform(X_test['tokens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_vectors[X_train_vectors.columns[-400:]].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append vectorized notes to train and test X dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf, X_test_tf = pd.concat([X_train.reset_index(drop=True), X_train_vectors], axis = 1),  pd.concat([X_test.reset_index(drop=True), X_test_vectors], axis = 1)\n",
    "\n",
    "# Drop text data\n",
    "X_train_tf.drop(['text', 'tokens'], axis = 1, inplace=True)\n",
    "X_test_tf.drop(['text', 'tokens'], axis = 1, inplace=True)\n",
    "\n",
    "# Check that the number of rows has not changed\n",
    "assert X_train_tf.shape[0] == X_train.shape[0], 'Train data frame shape has changed'\n",
    "assert X_test_tf.shape[0] == X_test.shape[0], 'Test data frame shape has changed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33061, 409)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE Balancing\n",
    "- Do some undersampling/oversampling on the training data for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33061, 409)\n",
      "(33061, 1)\n",
      "Original train dataset shape Counter({0: 30832, 1: 2229})\n",
      "Original test dataset shape Counter({0: 7716, 1: 550})\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tf.shape)\n",
    "print(y_train.shape)\n",
    "print('Original train dataset shape {}'.format(Counter(y_train['output_label'])))\n",
    "print('Original test dataset shape {}'.format(Counter(y_test['output_label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train dataset shape Counter({'output_label': 1})\n",
      "Original test dataset shape Counter({'output_label': 1})\n",
      "New train dataset shape Counter({1: 30741, 0: 12698})\n"
     ]
    }
   ],
   "source": [
    "print('Original train dataset shape {}'.format(Counter(y_train['output_label'])))\n",
    "print('Original test dataset shape {}'.format(Counter(y_test['output_label'])))\n",
    "\n",
    "\n",
    "def balancing(X, Y, undersample = None):\n",
    "    # Oversampling with SMOTE\n",
    "    smt = SMOTE(random_state=20)\n",
    "    if undersample:\n",
    "        smt = SMOTEENN(random_state=20)\n",
    "\n",
    "    X_new, Y_new = smt.fit_sample(X, Y)\n",
    "    print('New train dataset shape {}'.format(Counter(Y_new)))\n",
    "    X_new = pd.DataFrame(X_new, columns = list(X.columns))\n",
    "    return X_new, Y_new\n",
    "\n",
    "X_train_balanced, y_train_balanced = balancing(X_train_tf, y_train['output_label'], undersample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_balanced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "- Train 3 models:\n",
    "    1. Using only structural data\n",
    "    2. Using only notes data\n",
    "    3. Using all data features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building\n",
    "from scipy import interp\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def DTCGrid(X_train, X_test, Y_train, Y_test, model):\n",
    "    if model == 'lr':\n",
    "        pipeline = Pipeline([('clf',LogisticRegression(penalty = 'l2', max_iter=1000, random_state = 20000, solver='lbfgs'))])\n",
    "        param_dist = {'clf__C': [0.0001, 0.0005, 0.001, 0.0033, 0.0066, 0.01, 0.033, 0.066, 0.1, 0.33, 1, 3, 6, 10, 100]}\n",
    "    \n",
    "    if model == 'dt':\n",
    "        pipeline = Pipeline([('clf',DecisionTreeClassifier(criterion='entropy', random_state=20000))])\n",
    "        # specify parameters and distributions to sample from\n",
    "        param_dist = {'clf__max_depth': sp_randint(20, 30),\n",
    "                 'clf__min_samples_split': sp_randint(2, 11)\n",
    "                    }\n",
    "    if model == 'rf':\n",
    "        pipeline = Pipeline([('clf',RandomForestClassifier(criterion='entropy', random_state=20000))])\n",
    "        # specify parameters and distributions to sample from\n",
    "        param_dist = {'clf__max_depth': sp_randint(20, 30),\n",
    "                     'clf__max_features': sp_randint(1, X_train.shape[1]),\n",
    "                 'clf__min_samples_split': sp_randint(2, 11)\n",
    "                    }\n",
    "    # run randomized search\n",
    "    n_iter_search = 20\n",
    "    rand_search = RandomizedSearchCV(pipeline, param_distributions=param_dist, random_state=20000,\n",
    "                                    n_iter=n_iter_search, cv = 10, n_jobs=-1,verbose=1, scoring='recall')\n",
    "    rand_search.fit(X_train, Y_train)\n",
    "    print('Best score: %0.3f' % rand_search.best_score_)\n",
    "    print('Best parameters set:')\n",
    "    best_parameters = rand_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(param_dist.keys()):\n",
    "        print('\\t%s: %r' % (param_name, best_parameters[param_name]))\n",
    "    predictions = rand_search.predict(X_test)\n",
    "    print(classification_report(Y_test, predictions))\n",
    "    print(\"AUC is {0:.2f}\".format(roc_auc_score(Y_test, predictions)))\n",
    "    print(confusion_matrix(Y_test, predictions))\n",
    "    return rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 15 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:271: UserWarning:\n",
      "\n",
      "The total space of parameters 15 is smaller than n_iter=20. Running 15 iterations. For exhaustive searches, use GridSearchCV.\n",
      "\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    5.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.704\n",
      "Best parameters set:\n",
      "\tclf__C: 100\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.70      0.81      7716\n",
      "           1       0.12      0.58      0.20       550\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      8266\n",
      "   macro avg       0.54      0.64      0.50      8266\n",
      "weighted avg       0.90      0.69      0.77      8266\n",
      "\n",
      "AUC is 0.64\n",
      "[[5384 2332]\n",
      " [ 230  320]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:   10.3s finished\n"
     ]
    }
   ],
   "source": [
    "# With SMOTE and Undersampling\n",
    "best_estimator = DTCGrid(X_train_tf[feature_set_1], X_test_tf[feature_set_1], y_train['output_label'], y_test['output_label'], model = 'lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_estimator = DTCGrid(X_train_tf[feature_set_1], X_test_tf[feature_set_1], y_train['output_label'], y_test['output_label'], model = 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic regression on just the vectorized text data\n",
    "nlp_model = DTCGrid(X_train_tf[X_train_tf.columns[-400:]], X_test_tf[X_test_tf.columns[-400:]], y_train['output_label'], y_test['output_label'], model = 'lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 15 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:271: UserWarning:\n",
      "\n",
      "The total space of parameters 15 is smaller than n_iter=20. Running 15 iterations. For exhaustive searches, use GridSearchCV.\n",
      "\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   19.7s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.725\n",
      "Best parameters set:\n",
      "\tclf__C: 0.1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.68      0.80      7716\n",
      "           1       0.13      0.65      0.21       550\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      8266\n",
      "   macro avg       0.54      0.66      0.50      8266\n",
      "weighted avg       0.91      0.68      0.76      8266\n",
      "\n",
      "AUC is 0.66\n",
      "[[5247 2469]\n",
      " [ 195  355]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge. Increase the number of iterations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_model = DTCGrid(X_train_tf, X_test_tf, y_train['output_label'], y_test['output_label'], model = 'lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_estimator = DTCGrid(X_train_tf[X_train_tf.columns[-400:]], X_test_tf[X_test_tf.columns[-400:]], y_train, y_test, model = 'dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge. Increase the number of iterations.\n",
      "\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge. Increase the number of iterations.\n",
      "\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge. Increase the number of iterations.\n",
      "\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge. Increase the number of iterations.\n",
      "\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge. Increase the number of iterations.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score: 69.17%\n",
      "Test Set score: 68.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge. Increase the number of iterations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_lr = LR(C = 100, penalty = 'l2', class_weight='balanced', random_state = 3, solver=\"lbfgs\")\n",
    "\n",
    "\n",
    "print(\"Cross Validation Score: {:.2%}\".format(np.mean(cross_val_score(model_lr, X_train_tf, y_train['output_label'], cv=5))))\n",
    "# logreg.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "model_lr.fit(X_train_tf, y_train['output_label'])\n",
    "print(\"Test Set score: {:.2%}\".format(model_lr.score(X_test_tf, y_test['output_label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.68\n",
      "Precision is 0.12\n",
      "Recall is 0.61\n",
      "AUC is 0.65\n",
      "[[5322 2394]\n",
      " [ 212  338]]\n"
     ]
    }
   ],
   "source": [
    "y_test_preds = model_lr.predict(X_test_tf)\n",
    "\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_test, y_test_preds)))\n",
    "print(\"Precision is {0:.2f}\".format(precision_score(y_test, y_test_preds)))\n",
    "print(\"Recall is {0:.2f}\".format(recall_score(y_test, y_test_preds)))\n",
    "print(\"AUC is {0:.2f}\".format(roc_auc_score(y_test, y_test_preds)))\n",
    "\n",
    "print(confusion_matrix(y_test, y_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search cv \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# def get_lr_hyperparams(x, y, nfolds):\n",
    "#     scoring = {'AUC': 'roc_auc', \n",
    "#                'prec': 'precision',\n",
    "#                'recall': 'recall'}\n",
    "#     param_grid = {'C': [0.0001, 0.0005, 0.001, 0.0033, 0.0066, 0.01, 0.033, 0.066, 0.1, 0.33, 1, 3, 6, 10, 100]}\n",
    "#     grid_search = GridSearchCV(LR(penalty='l1', solver='saga', class_weight='balanced', random_state=5, max_iter=100), \n",
    "#                                param_grid, scoring=scoring, cv=nfolds, refit='recall')\n",
    "#     grid_search.fit(x, y)\n",
    "#     grid_search.best_params_\n",
    "#     return grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversample the minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 30832), (1, 30832)]\n"
     ]
    }
   ],
   "source": [
    "ros = RandomOverSampler(random_state=0)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train_tf, y_train['output_label'])\n",
    "\n",
    "X_resampled = pd.DataFrame(X_resampled, columns = X_train_tf.columns)\n",
    "\n",
    "print(sorted(Counter(y_resampled).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.68\n",
      "Precision is 0.12\n",
      "Recall is 0.62\n",
      "AUC is 0.65\n",
      "[[5316 2400]\n",
      " [ 211  339]]\n"
     ]
    }
   ],
   "source": [
    "y_test_preds = model_lr.predict(X_test_tf)\n",
    "\n",
    "print(\"Accuracy is {0:.2f}\".format(accuracy_score(y_test, y_test_preds)))\n",
    "print(\"Precision is {0:.2f}\".format(precision_score(y_test, y_test_preds)))\n",
    "print(\"Recall is {0:.2f}\".format(recall_score(y_test, y_test_preds)))\n",
    "print(\"AUC is {0:.2f}\".format(roc_auc_score(y_test, y_test_preds)))\n",
    "\n",
    "print(confusion_matrix(y_test, y_test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 2 models\n",
    "\n",
    "- Model 1: Using only structural features\n",
    "- Model 2: Adding in discharge notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 15 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:271: UserWarning:\n",
      "\n",
      "The total space of parameters 15 is smaller than n_iter=20. Running 15 iterations. For exhaustive searches, use GridSearchCV.\n",
      "\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.676\n",
      "Best parameters set:\n",
      "\tclf__C: 0.0001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.54      0.69      7716\n",
      "           1       0.09      0.67      0.17       550\n",
      "\n",
      "   micro avg       0.55      0.55      0.55      8266\n",
      "   macro avg       0.53      0.61      0.43      8266\n",
      "weighted avg       0.90      0.55      0.66      8266\n",
      "\n",
      "AUC is 0.61\n",
      "[[4196 3520]\n",
      " [ 181  369]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:   12.4s finished\n"
     ]
    }
   ],
   "source": [
    "struct_model = DTCGrid(X_resampled[feature_set_1], X_test_tf[feature_set_1], y_resampled, y_test['output_label'], model = 'lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:271: UserWarning:\n",
      "\n",
      "The total space of parameters 15 is smaller than n_iter=20. Running 15 iterations. For exhaustive searches, use GridSearchCV.\n",
      "\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 15 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   27.7s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 10.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.706\n",
      "Best parameters set:\n",
      "\tclf__C: 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.69      0.80      7716\n",
      "           1       0.12      0.62      0.21       550\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      8266\n",
      "   macro avg       0.54      0.65      0.51      8266\n",
      "weighted avg       0.91      0.68      0.76      8266\n",
      "\n",
      "AUC is 0.65\n",
      "[[5320 2396]\n",
      " [ 210  340]]\n"
     ]
    }
   ],
   "source": [
    "bootstrap_model = DTCGrid(X_resampled, X_test_tf, y_resampled, y_test['output_label'], model = 'lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:271: UserWarning:\n",
      "\n",
      "The total space of parameters 15 is smaller than n_iter=20. Running 15 iterations. For exhaustive searches, use GridSearchCV.\n",
      "\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 15 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   22.8s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  8.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.700\n",
      "Best parameters set:\n",
      "\tclf__C: 6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.67      0.79      7716\n",
      "           1       0.11      0.59      0.19       550\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      8266\n",
      "   macro avg       0.54      0.63      0.49      8266\n",
      "weighted avg       0.90      0.67      0.75      8266\n",
      "\n",
      "AUC is 0.63\n",
      "[[5203 2513]\n",
      " [ 224  326]]\n"
     ]
    }
   ],
   "source": [
    "notes_model = DTCGrid(X_resampled[X_resampled.columns[-400:]], X_test_tf[X_test_tf.columns[-400:]], y_resampled, y_test['output_label'], model = 'lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle all of the models we need for the dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save struct model\n",
    "# pickle.dump(struct_model, open('model1.pkl', 'wb'))\n",
    "\n",
    "# # save nlp model\n",
    "# pickle.dump(bootstrap_model, open('nlp_model.pkl', 'wb'))\n",
    "\n",
    "# save notes model\n",
    "pickle.dump(notes_model, open('./models/note_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nlp_model.pkl', 'rb') as inFile:\n",
    "    test_model = pickle.load(inFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(X_test_tf, open('X_test_tf.pkl', 'wb'))\n",
    "# pickle.dump(y_test, open('y_test.pkl', 'wb'))\n",
    "# pickle.dump(adm_processed, open('./test_data/adm_table.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try random forest on non-normalized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_norm_adm = pd.read_csv('../admission_processed.csv', parse_dates=['admittime', 'dischtime', 'deathtime', 'edregtime', 'edouttime', 'next_admittime', 'dob'], date_parser=pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_norm_adm = no_norm_adm[['hadm_id', 'subject_id', 'days_next_admit'] + feature_set_1]\n",
    "\n",
    "# Defining dictionaries for encoding\n",
    "admin_type_dict = {'EMERGENCY': 0, 'URGENT': 0, 'ELECTIVE': 1}\n",
    "gender_dict = {'M': 0, 'F': 1}\n",
    "\n",
    "# Mapping dictionaries to binary features\n",
    "no_norm_adm['admission_type'] = no_norm_adm['admission_type'].map(admin_type_dict).astype(int)\n",
    "no_norm_adm['gender'] = no_norm_adm['gender'].map(gender_dict).astype(int)\n",
    "\n",
    "# Generate output label for readmissions under 30 days\n",
    "no_norm_adm['output_label'] = (no_norm_adm['days_next_admit'] < 30).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>days_next_admit</th>\n",
       "      <th>admission_type</th>\n",
       "      <th>total_prior_admits</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>length_of_stay</th>\n",
       "      <th>num_medications</th>\n",
       "      <th>num_lab_tests</th>\n",
       "      <th>perc_tests_abnormal</th>\n",
       "      <th>num_diagnosis</th>\n",
       "      <th>output_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>185777</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>47.843943</td>\n",
       "      <td>7.759028</td>\n",
       "      <td>59</td>\n",
       "      <td>245.0</td>\n",
       "      <td>0.240816</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107064</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>65.938398</td>\n",
       "      <td>16.364583</td>\n",
       "      <td>148</td>\n",
       "      <td>573.0</td>\n",
       "      <td>0.448517</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>194540</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>50.146475</td>\n",
       "      <td>25.529167</td>\n",
       "      <td>91</td>\n",
       "      <td>442.0</td>\n",
       "      <td>0.104072</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>143045</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>39.863107</td>\n",
       "      <td>6.855556</td>\n",
       "      <td>84</td>\n",
       "      <td>359.0</td>\n",
       "      <td>0.298050</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>194023</td>\n",
       "      <td>17</td>\n",
       "      <td>128.920833</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>47.452430</td>\n",
       "      <td>4.368056</td>\n",
       "      <td>55</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.296875</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hadm_id  subject_id  days_next_admit  admission_type  total_prior_admits  \\\n",
       "0   185777           4              NaN               0                   0   \n",
       "1   107064           6              NaN               1                   0   \n",
       "2   194540          11              NaN               0                   0   \n",
       "3   143045          13              NaN               0                   0   \n",
       "4   194023          17       128.920833               1                   0   \n",
       "\n",
       "   gender        age  length_of_stay  num_medications  num_lab_tests  \\\n",
       "0       1  47.843943        7.759028               59          245.0   \n",
       "1       1  65.938398       16.364583              148          573.0   \n",
       "2       1  50.146475       25.529167               91          442.0   \n",
       "3       1  39.863107        6.855556               84          359.0   \n",
       "4       1  47.452430        4.368056               55          192.0   \n",
       "\n",
       "   perc_tests_abnormal  num_diagnosis  output_label  \n",
       "0             0.240816              9             0  \n",
       "1             0.448517              8             0  \n",
       "2             0.104072              1             0  \n",
       "3             0.298050              5             0  \n",
       "4             0.296875              4             0  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_norm_adm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33061, 9) (8266, 9) (33061, 1) (8266, 1)\n"
     ]
    }
   ],
   "source": [
    "# shuffle the samples\n",
    "no_norm_adm = no_norm_adm.sample(n = len(no_norm_adm), random_state=42)\n",
    "no_norm_adm.reset_index(drop=True, inplace=True)\n",
    "\n",
    "no_norm_target = no_norm_adm[['output_label']]\n",
    "no_norm_data = no_norm_adm[feature_set_1]\n",
    "\n",
    "nn_X_train, nn_X_test, nn_y_train, nn_y_test = train_test_split(no_norm_data, no_norm_target, test_size=0.2, random_state = 0)\n",
    "print(nn_X_train.shape, nn_X_test.shape, nn_y_train.shape, nn_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_X_train_tf, nn_X_test_tf = pd.concat([nn_X_train.reset_index(drop=True), X_train_vectors], axis=1), pd.concat([nn_X_test.reset_index(drop=True), X_test_vectors], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33061, 409) (8266, 409)\n"
     ]
    }
   ],
   "source": [
    "print(nn_X_train_tf.shape, nn_X_test_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 30832), (1, 30832)]\n"
     ]
    }
   ],
   "source": [
    "# Oversample trainset\n",
    "nn_X_resampled, nn_y_resampled = ros.fit_resample(nn_X_train_tf, nn_y_train['output_label'])\n",
    "\n",
    "nn_X_resampled = pd.DataFrame(nn_X_resampled, columns=nn_X_train_tf.columns)\n",
    "\n",
    "print(sorted(Counter(nn_y_resampled).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   17.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  1.5min finished\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning:\n",
      "\n",
      "The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 1.000\n",
      "Best parameters set:\n",
      "\tclf__max_depth: 28\n",
      "\tclf__max_features: 4\n",
      "\tclf__min_samples_split: 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96      7716\n",
      "           1       0.20      0.05      0.08       550\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      8266\n",
      "   macro avg       0.57      0.52      0.52      8266\n",
      "weighted avg       0.89      0.92      0.90      8266\n",
      "\n",
      "AUC is 0.52\n",
      "[[7598  118]\n",
      " [ 521   29]]\n"
     ]
    }
   ],
   "source": [
    "nn_bootstrap_model = DTCGrid(nn_X_resampled[feature_set_1], nn_X_test_tf[feature_set_1], nn_y_resampled, nn_y_test['output_label'], model = 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Admission Date:  [**2144-8-12**]       Discharge Date:  [**2144-8-20**]\\n\\nDate of Birth:   [**2084-4-14**]       Sex:  F\\n\\nService:\\n\\nHISTORY OF PRESENT ILLNESS:  The patient is a 60 year old,\\nright handed female who started experiencing nausea and\\nvomiting one week ago.  She felt dizzy and had diplopia.  She\\nfell the day prior to admission and was brought back to bed\\nand could not get out of bed until the day of admission when\\nEMS was called.  She was brought to [**Hospital3 4527**] Hospital\\nwhere a CT of her head showed left frontal hemorrhage and\\nright occipital hemorrhage.  Also a mass was detected on her\\nchest x-ray and confirmed by chest CT.  Patient was then\\ntransferred to [**Hospital1 69**].\\n\\nPAST MEDICAL HISTORY:  Lumbar diskectomy.  Hypertension.\\nDepression.\\n\\nALLERGIES:  None known.\\n\\nMEDICATIONS ON ADMISSION:  Aspirin 81 mg p.o. q.day, Univasc\\n30 mg q.day, thiamine 100 mg IM, MVI one p.o. q.day, Levaquin\\n500 mg IV q.24 hours, atenolol 75 mg p.o. q.a.m., Norvasc 10\\nmg p.o. q.day, folate 1 mg p.o. q.day, Tylenol and magnesium.\\n\\nPHYSICAL EXAMINATION:  The patient's heart rate was 46, blood\\npressure 158/51, SAO2 96 percent, respiratory rate 19.\\nPatient was awake, alert, slightly lethargic, oriented times\\ntwo.  Lungs were clear, left lung slightly coarse breath\\nsounds.  Abdomen soft, nontender.  Cardiac regular rate and\\nrhythm.  Extremities had no edema.  Neurologic exam showed an\\nawake, slightly lethargic, oriented times two, not to time,\\nslightly reversed when following commands.  Right eye had\\nslightly decreased adduction, slight decreased elevation of\\nthe right eye.  Also left lateral gaze nystagmus.  Face was\\nsymmetric.  Frontalis was intact.  Moderate dysarthria.  No\\nobvious drift.  Upper and lower extremities were full [**5-18**] for\\nmuscle.  Reflexes were 1+ in the upper extremities and 1 in\\nthe lower extremities.  No Hoffmann, no clonus.  Sensation\\nwas intact to lower extremities bilaterally.\\n\\nLABORATORY DATA:  White count 10.6, hematocrit 27.4,\\nplatelets 309.  INR 1.3, PTT 33.2.  Sodium 139, potassium\\n3.4, chloride 106, CO2 22.\\n\\nHOSPITAL COURSE:  The plan for this patient was admission to\\nthe ICU, obtain an MRI with gadolinium.  She was made NPO, IV\\nfluids at 80 an hour.  Neuro checks were q.one hour.  She was\\ngiven mannitol 25 q.four hours and Decadron 8 mg q.four\\nhours.  On [**8-13**] the patient was noted to be arousable,\\noriented to person only, very dysarthric, more lethargic than\\nthe previous day.  She had received Ativan at the time of\\nexamination.  She did have a slight right drift.  She had an\\nMRI which showed a large right cerebellar T2 hypointense and\\nT1 hyperintense mass which measured about 5 cm in maximal\\ndimension.  A DWI scan [**Month (only) 3780**] loss of signal in the\\nright cerebellum and left frontal lobe.  MR [**First Name (Titles) 4058**]\\n[**Last Name (Titles) 3780**] flow in both intracranial carotid arteries and\\nin the anterior middle cerebral artery.  There was flow in\\nthe vertebral arteries.  The impression was cerebellar and\\ncerebral hemorrhagic masses consistent with metastasis.  The\\ncerebellar mass is large and there is superior herniation\\ninto the cerebellum and compression of the fourth ventricle.\\n\\nGiven the patient's exam and MRI findings, it was felt that\\nthe patient should have an emergent craniotomy for removal of\\nthis tumor and the hemorrhagic lesion.  Prior to surgery\\npatient did have an interventional pulmonology consult.  They\\nsaw her and felt she had a left upper lobe mass infiltrate\\nwith the diagnosis that included possible pneumonia versus\\nlung mass versus combination.  They did follow her throughout\\nher stay here and did not recommend that she needed\\nbronchoscopy at this time.  They referred her to oncology.\\n\\nThe patient was brought to the O.R. on [**2144-8-13**] where she\\nunderwent craniotomy and removal of the right cerebellar\\nhemorrhagic lesion.  Patient did well intraoperatively and\\nhad no complications.  On post-op day one patient was awake,\\noriented times name only, inattentive, unable to follow\\nmidline commands.  She had abduction depression of the right\\neye.  Pupils were 5 to 3.5 in the right eye and left eye 4.5\\nto 2.5.  She had a question of bilateral slight pronator\\ndrift, greater on the right than the left.  Recommendations\\nwere to keep her blood pressure less than 150, use minimal\\nAtivan only and to slowly advance her diet.  Her post-op\\nhematocrit was 25.9.  Also on the 1st she had repeat MRI\\nwhich showed status post resection of right cerebellar\\nhemorrhagic lesion without residual enhancement.  Left\\nfrontal hemorrhagic lesion with enhancement was suggestive of\\nmetastatic disease.\\n\\nThe patient did require the use of Nipride to keep her\\nsystolic blood pressure less than 140.  She did have periods\\nof agitation at times.  On [**8-15**] patient was awake,\\ncontinued in the ICU.  Her face was symmetric.  No rebound,\\nno drift.  Finger to nose showed slight right dysmetria.\\nPatient was advanced out of bed to a chair.  Her activity was\\nalso increased.  Patient was continued on IV Decadron 4 mg\\nq.six hours.  In the late afternoon of the 2nd, patient was\\ntransferred out of the unit awake, alert and oriented times\\nthree, moving all extremities.  Vital signs were stable.  She\\ndid see physical therapy when moved to the floor who\\nrecommended that she receive visitations every day by them to\\nassist with her impaired mobility and help with her impaired\\nmental status.\\n\\nOn [**8-16**] the patient was awake, alert, oriented,\\ntolerating a diet, ambulating with help.  Her dysarthria was\\nnoted to be improved.  She had a slight right drift noted.\\nHer dysmetria was also improved.  Her Foley was removed.\\nDecadron was continued to be weaned.  Occupational therapy\\nalso saw her and assisted her with safety awareness and\\nhelped her with her upper extremities.  On the 5th oncology\\nsaw patient who felt that she most likely will receive\\npalliative chemotherapy with radiation rather than systemic\\nchemo.  Patient's special stains for melanoma were found to\\nbe negative on [**8-19**].  Patient will need to go to a rehab\\nfacility for further assistance with her gait and\\nrehabilitation needs.  She is to follow up in the brain tumor\\nclinic with Dr. [**Last Name (STitle) 724**].  She also needs to follow up with\\nhematology/oncology here at [**Hospital1 188**].\\n\\nThe patient was treated with Levaquin for pneumonia that was\\ndiagnosed prior to admission.  She received 10 days of\\nLevaquin therapy.\\n\\nDISCHARGE INSTRUCTIONS:  To keep the staples clean and dry,\\ndo not get them wet.  She should have them removed on\\n[**2144-8-26**].  She can come back to [**Hospital Ward Name 121**] 5 to have those removed\\nor she can have them removed at her rehab facility.  Right\\nnow her appointment for the brain tumor clinic is pending.\\nAlso her appointment with oncology is pending at this time.\\n\\nDISCHARGE MEDICATIONS:\\n1.  Heparin 5000 units subcu.\\n2.  Regular insulin sliding scale.\\n3.  Nicotine patch 20 mg per 24 hours.\\n4.  Lopressor 50 mg [**1-16**] tablet b.i.d.\\n5.  Protonix 40 mg q.day.\\n6.  Oxycodone with acetaminophen one to two tablets p.o.\\nq.four to six hours.\\n7.  Decadron taper down to 2 mg p.o. b.i.d.\\n\\n\\n\\n\\n\\n\\n                            [**First Name11 (Name Pattern1) 125**] [**Last Name (NamePattern4) 342**], M.D.  [**MD Number(1) 343**]\\n\\nDictated By:[**Last Name (NamePattern1) 13027**]\\nMEDQUIST36\\n\\nD:  [**2144-8-19**]  10:24\\nT:  [**2144-8-19**]  10:29\\nJOB#:  [**Job Number 51303**]\\n\""
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_dis_notes[last_dis_notes['hadm_id'] == 130744].text.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "1297.78px",
    "left": "133px",
    "top": "132.278px",
    "width": "369.41px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
