{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical NLP to predict readmission with discharge summaries<span class=\"tocSkip\"></span></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# For interacting with PostgreSQL database for mimic queries\n",
    "import psycopg2\n",
    "\n",
    "# Ploting functions\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "plotly.tools.set_credentials_file(username='mlpaff', api_key='lYV8hhGxZlP988tplymj')\n",
    "plotly.tools.set_config_file(world_readable=True,\n",
    "                             sharing='public')\n",
    "\n",
    "from IPython.core.pylabtools import figsize\n",
    "import matplotlib.pyplot as plt\n",
    "figsize(20, 10)\n",
    "plt.style.use(['dark_background'])\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from collections import Counter\n",
    "\n",
    "color_set = ['#A9CA59', '#6582C4', '#62C9BC', '#F58D50', '#2AD7F4',\n",
    "             '#AB3EED', '#FF6CB2', '#FFA466', '#FFE256', '#47EAAC', '#2AD7F4', '#3C8CF9']\n",
    "\n",
    "# specify user and database for SQL queries\n",
    "sqluser = 'mattmimic'\n",
    "dbname = 'mimic'\n",
    "set_schema = '--search_path=mimiciii'\n",
    "\n",
    "# Connect to the database\n",
    "# con = psycopg2.connect(dbname = dbname, user = sqluser, options = set_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Admissions-table\" data-toc-modified-id=\"Admissions-table-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Admissions table</a></span></li><li><span><a href=\"#NOTEEVENTS-table\" data-toc-modified-id=\"NOTEEVENTS-table-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>NOTEEVENTS table</a></span></li><li><span><a href=\"#Load-in-Andrew's-processed-admissions-table-for-merging-with-notes\" data-toc-modified-id=\"Load-in-Andrew's-processed-admissions-table-for-merging-with-notes-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Load in Andrew's processed admissions table for merging with notes</a></span><ul class=\"toc-item\"><li><span><a href=\"#Merge-notes-with-admissions-table\" data-toc-modified-id=\"Merge-notes-with-admissions-table-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Merge notes with admissions table</a></span></li></ul></li><li><span><a href=\"#Create-training-and-test-dataframes\" data-toc-modified-id=\"Create-training-and-test-dataframes-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Create training and test dataframes</a></span></li><li><span><a href=\"#Preprocess-text-data\" data-toc-modified-id=\"Preprocess-text-data-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Preprocess text data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-balancing-applied-using-SMOTE\" data-toc-modified-id=\"Data-balancing-applied-using-SMOTE-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Data balancing applied using SMOTE</a></span></li></ul></li><li><span><a href=\"#Build-a-tokenizer\" data-toc-modified-id=\"Build-a-tokenizer-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Build a tokenizer</a></span><ul class=\"toc-item\"><li><span><a href=\"#Customize-tokenizer\" data-toc-modified-id=\"Customize-tokenizer-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Customize tokenizer</a></span></li></ul></li><li><span><a href=\"#Build-a-simple-vectorizer\" data-toc-modified-id=\"Build-a-simple-vectorizer-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Build a simple vectorizer</a></span><ul class=\"toc-item\"><li><span><a href=\"#Build-a-vectorizer-on-clinical-notes\" data-toc-modified-id=\"Build-a-vectorizer-on-clinical-notes-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Build a vectorizer on clinical notes</a></span></li></ul></li><li><span><a href=\"#Build-simple-predictive-model\" data-toc-modified-id=\"Build-simple-predictive-model-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Build simple predictive model</a></span></li><li><span><a href=\"#Model-evaluation\" data-toc-modified-id=\"Model-evaluation-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Model evaluation</a></span></li><li><span><a href=\"#Word2Vec\" data-toc-modified-id=\"Word2Vec-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Word2Vec</a></span><ul class=\"toc-item\"><li><span><a href=\"#W2V-preprocessing-of-text\" data-toc-modified-id=\"W2V-preprocessing-of-text-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>W2V preprocessing of text</a></span><ul class=\"toc-item\"><li><span><a href=\"#Word-2-Vec-implementation\" data-toc-modified-id=\"Word-2-Vec-implementation-10.1.1\"><span class=\"toc-item-num\">10.1.1&nbsp;&nbsp;</span>Word 2 Vec implementation</a></span></li></ul></li></ul></li><li><span><a href=\"#Train-a-simple-model\" data-toc-modified-id=\"Train-a-simple-model-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Train a simple model</a></span></li><li><span><a href=\"#Model-evaluation\" data-toc-modified-id=\"Model-evaluation-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Model evaluation</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Admissions table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>admittime</th>\n",
       "      <th>dischtime</th>\n",
       "      <th>deathtime</th>\n",
       "      <th>admission_type</th>\n",
       "      <th>hospital_expire_flag</th>\n",
       "      <th>next_admittime</th>\n",
       "      <th>next_admit_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>2138-07-17 19:04:00</td>\n",
       "      <td>2138-07-21 15:48:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NEWBORN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>3</td>\n",
       "      <td>145834</td>\n",
       "      <td>2101-10-20 19:08:00</td>\n",
       "      <td>2101-10-31 13:58:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>4</td>\n",
       "      <td>185777</td>\n",
       "      <td>2191-03-16 00:28:00</td>\n",
       "      <td>2191-03-23 18:41:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>5</td>\n",
       "      <td>178980</td>\n",
       "      <td>2103-02-02 04:31:00</td>\n",
       "      <td>2103-02-04 12:15:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NEWBORN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>6</td>\n",
       "      <td>107064</td>\n",
       "      <td>2175-05-30 07:15:00</td>\n",
       "      <td>2175-06-15 16:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>ELECTIVE</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     subject_id  hadm_id           admittime           dischtime deathtime  \\\n",
       "212           2   163353 2138-07-17 19:04:00 2138-07-21 15:48:00       NaT   \n",
       "213           3   145834 2101-10-20 19:08:00 2101-10-31 13:58:00       NaT   \n",
       "214           4   185777 2191-03-16 00:28:00 2191-03-23 18:41:00       NaT   \n",
       "215           5   178980 2103-02-02 04:31:00 2103-02-04 12:15:00       NaT   \n",
       "216           6   107064 2175-05-30 07:15:00 2175-06-15 16:00:00       NaT   \n",
       "\n",
       "    admission_type  hospital_expire_flag next_admittime next_admit_type  \n",
       "212        NEWBORN                     0            NaT             NaN  \n",
       "213      EMERGENCY                     0            NaT             NaN  \n",
       "214      EMERGENCY                     0            NaT             NaN  \n",
       "215        NEWBORN                     0            NaT             NaN  \n",
       "216       ELECTIVE                     0            NaT             NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con = psycopg2.connect(dbname = dbname, user = sqluser, options = set_schema)\n",
    "query = 'SELECT subject_id, hadm_id, admittime, dischtime, deathtime, admission_type, hospital_expire_flag FROM admissions;'\n",
    "admissions = pd.read_sql_query(query, con, parse_dates=['admittime', 'dischtime', 'deathtime'])\n",
    "con.close()\n",
    "\n",
    "# we will drop all patients that had negative\n",
    "\n",
    "admissions.sort_values(['subject_id', 'admittime'], inplace=True)\n",
    "\n",
    "# Create a column for next admission time (if it exists)\n",
    "admissions['next_admittime'] = admissions.groupby('subject_id')['admittime'].shift(-1)\n",
    "# Get the next admission type\n",
    "admissions['next_admit_type'] = admissions.groupby('subject_id')['admission_type'].shift(-1)\n",
    "\n",
    "admissions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>admittime</th>\n",
       "      <th>dischtime</th>\n",
       "      <th>deathtime</th>\n",
       "      <th>admission_type</th>\n",
       "      <th>hospital_expire_flag</th>\n",
       "      <th>next_admittime</th>\n",
       "      <th>next_admit_type</th>\n",
       "      <th>days_next_admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>2138-07-17 19:04:00</td>\n",
       "      <td>2138-07-21 15:48:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NEWBORN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>3</td>\n",
       "      <td>145834</td>\n",
       "      <td>2101-10-20 19:08:00</td>\n",
       "      <td>2101-10-31 13:58:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>4</td>\n",
       "      <td>185777</td>\n",
       "      <td>2191-03-16 00:28:00</td>\n",
       "      <td>2191-03-23 18:41:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>5</td>\n",
       "      <td>178980</td>\n",
       "      <td>2103-02-02 04:31:00</td>\n",
       "      <td>2103-02-04 12:15:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NEWBORN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>6</td>\n",
       "      <td>107064</td>\n",
       "      <td>2175-05-30 07:15:00</td>\n",
       "      <td>2175-06-15 16:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>ELECTIVE</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     subject_id  hadm_id           admittime           dischtime deathtime  \\\n",
       "212           2   163353 2138-07-17 19:04:00 2138-07-21 15:48:00       NaT   \n",
       "213           3   145834 2101-10-20 19:08:00 2101-10-31 13:58:00       NaT   \n",
       "214           4   185777 2191-03-16 00:28:00 2191-03-23 18:41:00       NaT   \n",
       "215           5   178980 2103-02-02 04:31:00 2103-02-04 12:15:00       NaT   \n",
       "216           6   107064 2175-05-30 07:15:00 2175-06-15 16:00:00       NaT   \n",
       "\n",
       "    admission_type  hospital_expire_flag next_admittime next_admit_type  \\\n",
       "212        NEWBORN                     0            NaT             NaN   \n",
       "213      EMERGENCY                     0            NaT             NaN   \n",
       "214      EMERGENCY                     0            NaT             NaN   \n",
       "215        NEWBORN                     0            NaT             NaN   \n",
       "216       ELECTIVE                     0            NaT             NaN   \n",
       "\n",
       "     days_next_admit  \n",
       "212              NaN  \n",
       "213              NaN  \n",
       "214              NaN  \n",
       "215              NaN  \n",
       "216              NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out elective emissions since we are interested in uplanned re-admissions\n",
    "rows = admissions['next_admit_type'] == 'ELECTIVE'\n",
    "admissions.loc[rows, 'next_admittime'] = pd.NaT\n",
    "admissions.loc[rows, 'next_admit_type'] = np.NaN\n",
    "\n",
    "# Sort by subject_id and admission date\n",
    "admissions.sort_values(['subject_id', 'admittime'])\n",
    "\n",
    "# back fill\n",
    "admissions[['next_admittime', 'next_admit_type']] = admissions.groupby('subject_id')[['next_admittime', 'next_admit_type']].fillna(method='bfill')\n",
    "# Calculate the days until the next admission\n",
    "admissions['days_next_admit'] = (admissions.next_admittime - admissions.dischtime).dt.total_seconds() / (24*60*60)\n",
    "\n",
    "# Filter out the hospital admissions where the patient died. We know that rows will not have a \"next admission\" and wont add any useful information for predicting readmission rates\n",
    "admissions = admissions[admissions['hospital_expire_flag'] != 1]\n",
    "\n",
    "admissions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hist_data = [admissions.days_next_admit.dropna()]\n",
    "# # fig = ff.create_distplot(hist_data, group_labels=['Days between Admissions'], show_rug=False, bin_size=100)\n",
    "\n",
    "# trace = go.Histogram(\n",
    "#     x = admissions['days_next_admit']\n",
    "# )\n",
    "# layout = go.Layout(\n",
    "#     xaxis = dict(\n",
    "#         title = 'Days between Admissions'\n",
    "#     ),\n",
    "#     yaxis = dict(\n",
    "#         title = 'Count'\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# fig = go.Figure(data = [trace], layout = layout)\n",
    "\n",
    "# iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTEEVENTS table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>chartdate</th>\n",
       "      <th>charttime</th>\n",
       "      <th>storetime</th>\n",
       "      <th>category</th>\n",
       "      <th>description</th>\n",
       "      <th>cgid</th>\n",
       "      <th>iserror</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>174</td>\n",
       "      <td>22532</td>\n",
       "      <td>167853.0</td>\n",
       "      <td>2151-08-04</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Admission Date:  [**2151-7-16**]       Dischar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175</td>\n",
       "      <td>13702</td>\n",
       "      <td>107527.0</td>\n",
       "      <td>2118-06-14</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Admission Date:  [**2118-6-2**]       Discharg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176</td>\n",
       "      <td>13702</td>\n",
       "      <td>167118.0</td>\n",
       "      <td>2119-05-25</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Admission Date:  [**2119-5-4**]              D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>177</td>\n",
       "      <td>13702</td>\n",
       "      <td>196489.0</td>\n",
       "      <td>2124-08-18</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Admission Date:  [**2124-7-21**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>178</td>\n",
       "      <td>26880</td>\n",
       "      <td>135453.0</td>\n",
       "      <td>2162-03-25</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Admission Date:  [**2162-3-3**]              D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  subject_id   hadm_id  chartdate charttime storetime  \\\n",
       "0     174       22532  167853.0 2151-08-04       NaT       NaT   \n",
       "1     175       13702  107527.0 2118-06-14       NaT       NaT   \n",
       "2     176       13702  167118.0 2119-05-25       NaT       NaT   \n",
       "3     177       13702  196489.0 2124-08-18       NaT       NaT   \n",
       "4     178       26880  135453.0 2162-03-25       NaT       NaT   \n",
       "\n",
       "            category description  cgid iserror  \\\n",
       "0  Discharge summary      Report   NaN    None   \n",
       "1  Discharge summary      Report   NaN    None   \n",
       "2  Discharge summary      Report   NaN    None   \n",
       "3  Discharge summary      Report   NaN    None   \n",
       "4  Discharge summary      Report   NaN    None   \n",
       "\n",
       "                                                text  \n",
       "0  Admission Date:  [**2151-7-16**]       Dischar...  \n",
       "1  Admission Date:  [**2118-6-2**]       Discharg...  \n",
       "2  Admission Date:  [**2119-5-4**]              D...  \n",
       "3  Admission Date:  [**2124-7-21**]              ...  \n",
       "4  Admission Date:  [**2162-3-3**]              D...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "con = psycopg2.connect(dbname = dbname, user = sqluser, options = set_schema)\n",
    "query = 'SELECT * FROM noteevents;'\n",
    "notes = pd.read_sql_query(query, con)\n",
    "con.close()\n",
    "notes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to discharge summary notes only\n",
    "dis_notes = notes[notes['category'] == 'Discharge summary'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Multiple discharge summaries per admission",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-750e31970038>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mdis_notes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hadm_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Multiple discharge summaries per admission'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Multiple discharge summaries per admission"
     ]
    }
   ],
   "source": [
    "assert dis_notes.duplicated(['hadm_id']).sum() == 0, 'Multiple discharge summaries per admission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab just the last discharge summaries by hadm_id\n",
    "last_dis_notes = dis_notes.groupby(['subject_id', 'hadm_id']).nth(-1).reset_index()\n",
    "assert last_dis_notes.duplicated(['hadm_id']).sum() == 0, 'Multiple discharge summaries per admission'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Andrew's processed admissions table for merging with notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "row_id                           int64\n",
       "subject_id                       int64\n",
       "hadm_id                          int64\n",
       "admittime               datetime64[ns]\n",
       "dischtime               datetime64[ns]\n",
       "deathtime               datetime64[ns]\n",
       "admission_type                  object\n",
       "admission_location              object\n",
       "discharge_location              object\n",
       "insurance                       object\n",
       "language                        object\n",
       "religion                        object\n",
       "marital_status                  object\n",
       "ethnicity                       object\n",
       "edregtime                       object\n",
       "edouttime                       object\n",
       "diagnosis                       object\n",
       "hospital_expire_flag             int64\n",
       "has_chartevents_data             int64\n",
       "total_prior_admits               int64\n",
       "next_admittime          datetime64[ns]\n",
       "next_admit_type                 object\n",
       "days_next_admit                float64\n",
       "num_medications                  int64\n",
       "num_lab_tests                  float64\n",
       "perc_tests_abnormal            float64\n",
       "num_diagnosis                    int64\n",
       "gender                          object\n",
       "dob                             object\n",
       "age                            float64\n",
       "length_of_stay                 float64\n",
       "output_label                     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adm_processed = pd.read_csv('../admission_processed.csv', parse_dates=['admittime', 'dischtime', 'deathtime', 'next_admittime'], date_parser=pd.to_datetime)\n",
    "adm_processed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52726, 11)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_dis_notes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge notes with admissions table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "row_id_x                         int64\n",
       "subject_id                       int64\n",
       "hadm_id                          int64\n",
       "admittime               datetime64[ns]\n",
       "dischtime               datetime64[ns]\n",
       "deathtime               datetime64[ns]\n",
       "admission_type                  object\n",
       "admission_location              object\n",
       "discharge_location              object\n",
       "insurance                       object\n",
       "language                        object\n",
       "religion                        object\n",
       "marital_status                  object\n",
       "ethnicity                       object\n",
       "edregtime                       object\n",
       "edouttime                       object\n",
       "diagnosis                       object\n",
       "hospital_expire_flag             int64\n",
       "has_chartevents_data             int64\n",
       "total_prior_admits               int64\n",
       "next_admittime          datetime64[ns]\n",
       "next_admit_type                 object\n",
       "days_next_admit                float64\n",
       "num_medications                  int64\n",
       "num_lab_tests                  float64\n",
       "perc_tests_abnormal            float64\n",
       "num_diagnosis                    int64\n",
       "gender                          object\n",
       "dob                             object\n",
       "age                            float64\n",
       "length_of_stay                 float64\n",
       "output_label                     int64\n",
       "category                        object\n",
       "cgid                           float64\n",
       "chartdate               datetime64[ns]\n",
       "charttime               datetime64[ns]\n",
       "description                     object\n",
       "iserror                         object\n",
       "row_id_y                       float64\n",
       "storetime               datetime64[ns]\n",
       "text                            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adm_notes = admissions.merge(last_dis_notes, on = ['subject_id', 'hadm_id'], how = 'left')\n",
    "\n",
    "# Merge notes table with Andrew's processed table\n",
    "adm_notes = adm_processed.merge(last_dis_notes, on = ['subject_id', 'hadm_id'], how = 'left')\n",
    "\n",
    "# assert len(admissions) == len(adm_notes), 'Number of rows increased'\n",
    "\n",
    "# Remove Newborn admissions\n",
    "adm_notes = adm_notes[adm_notes['admission_type'] != 'NEWBORN']\n",
    "\n",
    "# Generate output label for readmissions under 30 days\n",
    "adm_notes['output_label'] = (adm_notes['days_next_admit'] < 30).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.023156774021825925"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adm_notes.text.isnull().sum() / len(adm_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EMERGENCY', 'ELECTIVE', 'URGENT']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(adm_notes.admission_type.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41327, 41)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adm_notes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of patients that were re-admitted within 30 days: 2779\n",
      "fraction of patients re-admitted within 30 days: 0.0672441745106105\n"
     ]
    }
   ],
   "source": [
    "print('number of patients that were re-admitted within 30 days:', len(adm_notes[adm_notes['output_label'] == 1]))\n",
    "print('fraction of patients re-admitted within 30 days:', len(adm_notes[adm_notes['output_label'] == 1]) / len(adm_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45321, 20)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training and test dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation and test sets\n",
    "\n",
    "# shuffle the samples\n",
    "adm_notes = adm_notes.sample(n = len(adm_notes), random_state=42)\n",
    "adm_notes.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = adm_notes[['output_label']]\n",
    "data = adm_notes[['subject_id', 'hadm_id', 'text']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df):\n",
    "    ''' Preprocesses the text by filling not a number and replacing new lines ('\\n') and carriage returns ('\\r')\n",
    "    '''\n",
    "    \n",
    "    df['text'] = df['text'].fillna(' ')\n",
    "    df['text'] = df['text'].str.replace('\\n', ' ')\n",
    "    df['text'] = df['text'].str.replace('\\r', ' ')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27144</th>\n",
       "      <td>7363</td>\n",
       "      <td>168193</td>\n",
       "      <td>Name:  [**Known lastname 3567**],[**Known firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26759</th>\n",
       "      <td>9906</td>\n",
       "      <td>175051</td>\n",
       "      <td>Admission Date:  [**2103-2-7**]              D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3656</th>\n",
       "      <td>23739</td>\n",
       "      <td>157370</td>\n",
       "      <td>Admission Date:  [**2127-5-7**]              D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>591</td>\n",
       "      <td>154785</td>\n",
       "      <td>Admission Date:  [**2161-12-1**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21686</th>\n",
       "      <td>77034</td>\n",
       "      <td>176891</td>\n",
       "      <td>Admission Date:  [**2150-11-9**]              ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject_id  hadm_id                                               text\n",
       "27144        7363   168193  Name:  [**Known lastname 3567**],[**Known firs...\n",
       "26759        9906   175051  Admission Date:  [**2103-2-7**]              D...\n",
       "3656        23739   157370  Admission Date:  [**2127-5-7**]              D...\n",
       "322           591   154785  Admission Date:  [**2161-12-1**]              ...\n",
       "21686       77034   176891  Admission Date:  [**2150-11-9**]              ..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, X_train = preprocess_text(X_test), preprocess_text(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data balancing applied using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using only the features in the feature list for the input\n",
    "# train_input = X[feature_list]\n",
    "# train_output = y\n",
    "\n",
    "# Splitting before SMOTE\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(train_input, train_output, test_size=0.20, random_state=0)\n",
    "print('Original train dataset shape {}'.format(Counter(y_train)))\n",
    "print('Original test dataset shape {}'.format(Counter(y_train)))\n",
    "\n",
    "\n",
    "def balancing(X, Y, undersample = None):\n",
    "    # Oversampling with SMOTE\n",
    "    smt = SMOTE(random_state=20)\n",
    "    if undersample:\n",
    "        smt = SMOTEENN(random_state=20)\n",
    "\n",
    "    X_new, Y_new = smt.fit_sample(X, Y)\n",
    "    print('New train dataset shape {}'.format(Counter(Y_new)))\n",
    "    X_new = pd.DataFrame(X_new, columns = list(X.columns))\n",
    "    return X_new, Y_new\n",
    "\n",
    "X_train_balanced, y_train_balanced = balancing(X_train, y_train, undersample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'should',\n",
       " 'be',\n",
       " 'tokenized',\n",
       " '.',\n",
       " '11/14/2018',\n",
       " 'sentence',\n",
       " 'has',\n",
       " 'stars',\n",
       " '**']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize('This should be tokenized. 11/14/2018 sentence has stars **')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize tokenizer \n",
    "- replace punctuation with spaces\n",
    "- remove numbers with spaces\n",
    "- lowercase all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeBetter(text):\n",
    "    ''' Tokenize the text by replacing punctuations and numbers with spaces and lowercase all words\n",
    "    '''\n",
    "    punc_list = string.punctuation + '0123456789'\n",
    "    t = str.maketrans(dict.fromkeys(punc_list, ' '))\n",
    "    text = text.lower().translate(t)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizeBetter('This should be tokenized. 11/14/2018 sentence has stars***')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a simple vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = ['Data science is about the data', 'The science is amazing', 'Predictive modeling is part of data science']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(tokenizer = tokenizeBetter)\n",
    "vect.fit(sample_text)\n",
    "\n",
    "# matrix is stored as a sparse matrix (since you have a lot of zeros)\n",
    "X = vect.transform(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the column names\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a vectorizer on clinical notes\n",
    "- Using the tokenizer we defined above as part of the pipeline, we can vectorize at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = ['the','and','to','of','was','with','a','on','in','for','name',\n",
    "                 'is','patient','s','he','at','as','or','one','she','his','her','am',\n",
    "                 'were','you','pt','pm','by','be','had','your','this','date',\n",
    "                'from','there','an','that','p','are','have','has','h','but','o',\n",
    "                'namepattern','which','every','also']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(max_features = 3000, \n",
    "                       tokenizer = tokenizeBetter,\n",
    "                       stop_words = my_stop_words)\n",
    "\n",
    "vect.fit(X_train['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf = vect.transform(X_train['text'].values)\n",
    "X_train_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build simple predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "clf = LR(C = 0.0001, penalty = 'l2', random_state = 3, class_weight=\"balanced\")\n",
    "clf.fit(X_train_tf, y_train)\n",
    "\n",
    "model = clf\n",
    "\n",
    "y_train_preds = model.predict_proba(X_train_tf)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "def calc_accuracy(y_actual, y_pred, thresh):\n",
    "    # this function calculates the accuracy with probability threshold at thresh\n",
    "    return (sum((y_pred > thresh) & (y_actual == 1))+sum((y_pred < thresh) & (y_actual == 0))) /len(y_actual)\n",
    "\n",
    "def calc_recall(y_actual, y_pred, thresh):\n",
    "    # calculates the recall\n",
    "    return sum((y_pred > thresh) & (y_actual == 1)) /sum(y_actual)\n",
    "\n",
    "def calc_precision(y_actual, y_pred, thresh):\n",
    "    # calculates the precision\n",
    "    return sum((y_pred > thresh) & (y_actual == 1)) /sum(y_pred > thresh)\n",
    "\n",
    "def calc_specificity(y_actual, y_pred, thresh):\n",
    "    # calculates specificity\n",
    "    return sum((y_pred < thresh) & (y_actual == 0)) /sum(y_actual ==0)\n",
    "\n",
    "def calc_prevalence(y_actual):\n",
    "    # calculates prevalence\n",
    "    return sum((y_actual == 1)) /len(y_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_train_preds)\n",
    "\n",
    "# thresh = 0.5\n",
    "\n",
    "# auc_train = roc_auc_score(y_train, y_train_preds)\n",
    "# print('Train AUC: %.3f'%auc_train)\n",
    "\n",
    "# print('Train accuracy:%.3f'%calc_accuracy(y_train, y_train_preds, thresh))\n",
    "# print('Train recall:%.3f'%calc_recall(y_train, y_train_preds, thresh))\n",
    "# print('Train precision:%.3f'%calc_precision(y_train, y_train_preds, thresh))\n",
    "# print('Train specificity:%.3f'%calc_specificity(y_train, y_train_preds, thresh))\n",
    "# print('Train prevalence:%.3f'%calc_prevalence(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr_train, tpr_train,'r-', label = 'Train AUC: %.2f'%auc_train)\n",
    "# plt.plot(fpr_valid, tpr_valid,'b-',label = 'Valid AUC: %.2f'%auc_valid)\n",
    "plt.plot([0,1],[0,1],'-k')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mattpaff/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import word2vec\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W2V preprocessing of text\n",
    "- Here we want to convert everything to lowercase and convert to list of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = ['the','and','to','of','was','with','a','on','in','for','name',\n",
    "                 'is','patient','s','he','at','as','or','one','she','his','her','am',\n",
    "                 'were','you','pt','pm','by','be','had','your','this','date',\n",
    "                'from','there','an','that','p','are','have','has','h','but','o',\n",
    "                'namepattern','which','every','also', 'b', 'i', 'd', 'admission', 'q', 't']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2vTokenizer(sentence):\n",
    "    ''' Tokenize the text by replacing punctuations and numbers with spaces and lowercase all words\n",
    "    '''\n",
    "    punc_list = string.punctuation + '0123456789'\n",
    "    t = str.maketrans(dict.fromkeys(punc_list, ' '))\n",
    "    text = str(sentence).lower().translate(t)\n",
    "    tokens = [x for x in word_tokenize(text.strip()) if x not in my_stop_words]\n",
    "#     tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sentence_to_wordlist(sentence, remove_stopwords=False):\n",
    "#     # remove non-letters\n",
    "#     sentence_text = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "#     #convert words to lowercase and split them\n",
    "#     words = sentence_text.lower().split()\n",
    "    \n",
    "#     return words\n",
    "\n",
    "def notes_to_sentences(notes, tokenizer, remove_stopwords=False):\n",
    "    ''' Split text data into tokenized list of sentences\n",
    "    '''\n",
    "    try:\n",
    "        # use NLTK tokenizer to split the text into sentences\n",
    "        raw_sentences = tokenizer.tokenize(notes)\n",
    "        \n",
    "        # Loop over each sentence\n",
    "        sentences = []\n",
    "        for sent in raw_sentences:\n",
    "            # if sentence is empty, skip it\n",
    "            if len(sent) > 0:\n",
    "                tokens = [x for x in w2vTokenizer(sent.strip()) if x not in my_stop_words]\n",
    "                if len(tokens) > 0:\n",
    "                    sentences.append(tokens)\n",
    "        # Return the list of sentences\n",
    "        return sentences\n",
    "    except:\n",
    "        print('nope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes_to_sentences(notes_list[0], tokenizer=tokenizer)\n",
    "# notes_list[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes_to_sentences(notes_list[0], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareW2Vtext(notes_list):\n",
    "    sentences = []\n",
    "    for note in notes_list:\n",
    "        note = str(note)\n",
    "        if len(note) > 0:\n",
    "            sentences += notes_to_sentences(note, tokenizer)\n",
    "    return(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_list = list(X_train['text'])\n",
    "processed_text = prepareW2Vtext(notes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3354537"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train['tokens'] = X_train['text'].apply(lambda x: tokenizeBetter(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word 2 Vec implementation\n",
    "- Train the W2V model on the processed list of text from patient notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigramer = gensim.models.Phrases(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 400      # Word vector dimentionality\n",
    "min_word_count = 50     # min word count\n",
    "num_workers = 4         # number of threads to run in parallel\n",
    "context = 4             # Context window size\n",
    "downsampling = 1e-3     # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(processed_text, workers=num_workers, \\\n",
    "                          size=num_features, min_count=min_word_count, \\\n",
    "                          window=context, sample=downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('mimic_w2v_model.bin')\n",
    "\n",
    "# Load model\n",
    "# model = Word2Vec.load('model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = dict(zip(model.wv.index2word, model.wv.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(sentence):\n",
    "    ''' Tokenize the text by replacing punctuations and numbers with spaces and lowercase all words\n",
    "    '''\n",
    "    punc_list = string.punctuation + '0123456789'\n",
    "    t = str.maketrans(dict.fromkeys(punc_list, ' '))\n",
    "    text = str(sentence).lower().translate(t)\n",
    "#     tokens = (x for x in word_tokenize(text.strip()) if x not in my_stop_words)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MeanEmbeddingVectorizer(object):\n",
    "#     def __init__(self, word2vec):\n",
    "#         self.word2vec = word2vec\n",
    "#         # if a text is empty we should return a vector of zeros\n",
    "#         # with the same dimensionality as all the other vectors\n",
    "#         self.dim = len(word2vec.wv.vectors)\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         return np.array([\n",
    "#             np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "#                     or [np.zeros(self.dim)], axis=0)\n",
    "#             for words in X\n",
    "#         ])\n",
    "\n",
    "class MyTokenizer(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        transformed_X = []\n",
    "        for document in X:\n",
    "            doc = [word for word in document if word in self.vocab]\n",
    "            transformed_X.append(doc)\n",
    "        return transformed_X\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "            \n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.wv.vectors)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "#         doc = [word for word in X if word in self.word2vec.wv.vocab]\n",
    "        X = MyTokenizer(self.word2vec.wv.vocab).fit_transform(X)\n",
    "    \n",
    "        return np.array([\n",
    "                    np.mean(self.word2vec[doc], axis = 0) for doc in X\n",
    "        ])\n",
    "#         return np.mean(self.word2vec[doc], axis = 0)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizor = MeanEmbeddingVectorizer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train['tokens'] = test_train.text.apply(lambda x: test(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the tokenized text to 400 dimension array\n",
    "test_train_tf = vectorizor.fit_transform(test_train['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf = pd.DataFrame(test_train_tf)\n",
    "X_train_tf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "lr = LR(C = 0.001, penalty = 'l2', random_state = 3, class_weight=\"balanced\")\n",
    "lr.fit(X_train_tf, y_train['output_label'])\n",
    "\n",
    "model = lr\n",
    "\n",
    "# y_train_preds = model.predict_proba(X_train_tf)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds = model.predict_proba(X_train_tf)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_train_preds)\n",
    "thresh = 0.5\n",
    "\n",
    "auc_train = roc_auc_score(y_train, y_train_preds)\n",
    "print('Train AUC: %.3f'%auc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['tokens'] = X_test['text'].apply(lambda x: process_text(x))\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tf = pd.DataFrame(vectorizor.fit_transform(X_test['tokens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds = model.predict_proba(X_test_tf)[:,1]\n",
    "\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_test_preds)\n",
    "thresh = 0.5\n",
    "\n",
    "auc_test = roc_auc_score(y_test, y_test_preds)\n",
    "print('Test AUC: %.3f'%auc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr_train, tpr_train,'r-', label = 'Train AUC: %.2f'%auc_train)\n",
    "# plt.plot(fpr_valid, tpr_valid,'b-',label = 'Valid AUC: %.2f'%auc_valid)\n",
    "plt.plot([0,1],[0,1],'-k')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr_test, tpr_test,'r-', label = 'Train AUC: %.2f'%auc_test)\n",
    "# plt.plot(fpr_valid, tpr_valid,'b-',label = 'Valid AUC: %.2f'%auc_valid)\n",
    "plt.plot([0,1],[0,1],'-k')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "1297.78px",
    "left": "309px",
    "top": "176.924px",
    "width": "297.431px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
